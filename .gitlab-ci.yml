stages:
  - validate
  - infrastructure
  - deploy
  - test

variables:
  CLUSTER_NAME: "axual-demo-cluster"
  AWS_REGION: "us-east-1"
  MYSQL_ROOT_PASSWORD: "DemoPassword123!" 

image: alpine:latest

before_script: |
  apk add --no-cache python3 py3-pip curl wget unzip bash ca-certificates
  pip3 install awscli --break-system-packages
  curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
  chmod +x kubectl && mv kubectl /usr/local/bin/
  curl -LO "https://get.helm.sh/helm-v3.17.0-linux-amd64.tar.gz"
  tar -zxvf helm-v3.17.0-linux-amd64.tar.gz && mv linux-amd64/helm /usr/local/bin/
  wget -O terraform.zip "https://releases.hashicorp.com/terraform/1.9.0/terraform_1.9.0_linux_amd64.zip"
  unzip -o terraform.zip -d /usr/local/bin/
  chmod +x /usr/local/bin/terraform
  aws configure set aws_access_key_id ${AWS_ACCESS_KEY_ID}
  aws configure set aws_secret_access_key ${AWS_SECRET_ACCESS_KEY}
  aws configure set region ${AWS_REGION}
  aws sts get-caller-identity

validate_terraform:
  stage: validate
  when: manual
  script: |
    cd terraform
    terraform init
    terraform validate
    set +e
    terraform plan -out=plan.tfplan -detailed-exitcode
    plan_exit=$?
    set -e
    if [ "$plan_exit" -eq 1 ]; then
      echo "Terraform plan failed due to configuration or credentials error."
      exit 1
    elif [ "$plan_exit" -ne 0 ] && [ "$plan_exit" -ne 2 ]; then
      echo "Terraform planning failed with unexpected exit code: $plan_exit."
      exit 1
    fi
    echo "Terraform plan succeeded(Exit code $plan_exit)."
  artifacts:
    paths:
      - terraform/plan.tfplan
    expire_in: 1 hour
  only:
    - main
    - merge_requests

deploy_infrastructure:
  stage: infrastructure
  when: manual
  script: |
    cd terraform
    terraform init
    terraform apply -input=false plan.tfplan
    echo "Waiting for EKS cluster to reach ACTIVE state..."
    aws eks wait cluster-active --name ${CLUSTER_NAME} --region ${AWS_REGION} || { 
      echo "EKS cluster failed to become active or timed out."
      exit 1
    }
    aws eks update-kubeconfig --region ${AWS_REGION} --name ${CLUSTER_NAME} --alias ${CLUSTER_NAME}
    kubectl cluster-info
    kubectl get nodes
  dependencies:
    - validate_terraform
  only:
    - main

deploy_prerequisites:
  stage: deploy
  script: |
    helm repo add eks https://aws.github.io/eks-charts
    helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
    helm repo add mysql-operator https://mysql.github.io/mysql-operator/
    helm repo add bitnami https://charts.bitnami.com/bitnami
    helm repo update
    aws eks update-kubeconfig --region ${AWS_REGION} --name ${CLUSTER_NAME}
    EBS_ROLE_ARN=$(aws iam get-role --role-name "${CLUSTER_NAME}-ebs-csi-driver" --query 'Role.Arn' --output text)
    ALB_ROLE_ARN=$(aws iam get-role --role-name "${CLUSTER_NAME}-alb-controller-role" --query 'Role.Arn' --output text)
    helm upgrade --install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver --namespace kube-system \
      --set controller.serviceAccount.create=true \
      --set controller.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="$EBS_ROLE_ARN" \
      --wait --timeout 5m
    helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller --namespace kube-system \
      --set clusterName=${CLUSTER_NAME} \
      --set region=${AWS_REGION} \
      --set serviceAccount.create=true \
      --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="$ALB_ROLE_ARN" \
      --wait --timeout 5m
    timeout 300s bash -c 'until kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver -o name 2>/dev/null | grep -q pod/; do echo "Waiting for aws-ebs-csi-driver pods..."; sleep 10; done'
    kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system --timeout=300s
    timeout 300s bash -c 'until kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o name 2>/dev/null | grep -q pod/; do echo "Waiting for aws-load-balancer-controller pods..."; sleep 10; done'
    kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=300s
  dependencies:
    - deploy_infrastructure
  only:
    - main

deploy_mysql:
  stage: deploy
  script: |
    helm repo add mysql-operator https://mysql.github.io/mysql-operator/
    helm repo update
    aws eks update-kubeconfig --region ${AWS_REGION} --name ${CLUSTER_NAME}
    
    for i in 1 2 3; do 
      helm upgrade --install mysql-operator mysql-operator/mysql-operator \
        --namespace mysql-operator \
        --create-namespace \
        --wait --timeout 5m && break || sleep 30
    done
    
    echo "Waiting for MySQL Operator pods to be created..."
    timeout 300s bash -c 'until kubectl get pods -n mysql-operator -l app.kubernetes.io/name=mysql-operator -o name 2>/dev/null | grep -q pod/; do echo "Waiting for operator pods..."; sleep 5; done'
    
    kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=mysql-operator -n mysql-operator --timeout=300s
    
    echo "Installing MySQL InnoDBCluster..."
    helm upgrade --install my-mysql-innodbcluster mysql-operator/mysql-innodbcluster \
      --namespace mysql-operator \
      --set credentials.root.password="${MYSQL_ROOT_PASSWORD}" \
      --set database="wordpress" \
      --set tls.useSelfSigned=true \
      --set instances=1 \
      --timeout 15m
    
    echo "Waiting for MySQL InnoDBCluster pods to be created..."
    timeout 600s bash -c 'until kubectl get pods -n mysql-operator -l app.kubernetes.io/instance=my-mysql-innodbcluster -o name 2>/dev/null | grep -q pod/; do echo "Waiting for MySQL pods..."; sleep 10; done'
    
    echo "Waiting for MySQL pods to be ready..."
    kubectl wait --for=condition=ready pod -n mysql-operator -l app.kubernetes.io/instance=my-mysql-innodbcluster --timeout=600s
    
    echo "Waiting for MySQL to be fully ready..."
    sleep 30
    
    echo "Initializing WordPress database and user..."
    for i in {1..10}; do
      if kubectl run -n mysql-operator mysql-init-$i \
        --image=mysql:8.0 \
        --rm \
        --restart=Never \
        --command -- \
        mysql -h my-mysql-innodbcluster.mysql-operator.svc.cluster.local \
        -u root \
        -p${MYSQL_ROOT_PASSWORD} \
        -e "CREATE DATABASE IF NOT EXISTS wordpress; CREATE USER IF NOT EXISTS 'wordpress'@'%' IDENTIFIED BY 'wordpress123'; GRANT ALL PRIVILEGES ON wordpress.* TO 'wordpress'@'%'; FLUSH PRIVILEGES;"; then
        echo "Database initialization successful!"
        break
      else
        echo "Database initialization attempt $i failed, retrying in 10 seconds..."
        sleep 10
      fi
    done
    
    echo "Verifying database connectivity..."
    kubectl run -n mysql-operator mysql-verify \
      --image=mysql:8.0 \
      --rm \
      --restart=Never \
      --command -- \
      mysql -h my-mysql-innodbcluster.mysql-operator.svc.cluster.local \
      -u wordpress \
      -pwordpress123 \
      -e "SHOW DATABASES;"
  dependencies:
    - deploy_prerequisites
  only:
    - main
  retry:
    max: 2
    when:
      - script_failure

deploy_wordpress:
  stage: deploy
  script: |
    helm repo add bitnami https://charts.bitnami.com/bitnami
    helm repo update
    aws eks update-kubeconfig --region ${AWS_REGION} --name ${CLUSTER_NAME}
    
    echo "Verifying MySQL connectivity before WordPress deployment..."
    kubectl run -n mysql-operator mysql-check \
      --image=mysql:8.0 \
      --rm \
      --restart=Never \
      --command -- \
      mysql -h my-mysql-innodbcluster.mysql-operator.svc.cluster.local \
      -u wordpress \
      -pwordpress123 \
      -e "SELECT 1;"
    
    echo "Deploying WordPress..."
    helm upgrade --install wordpress bitnami/wordpress \
      --namespace default \
      --values helm/wordpress-values.yaml \
      --set ingress.hostname=wordpress.${CI_PROJECT_PATH_SLUG}.demo.axual.com \
      --wait --timeout 15m
    
    echo "Waiting for WordPress pods to be created..."
    timeout 600s bash -c 'until kubectl get pods -l app.kubernetes.io/name=wordpress -o name 2>/dev/null | grep -q pod/; do echo "Waiting for WordPress pods..."; sleep 10; done'
    
    echo "Waiting for WordPress pods to be ready..."
    kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=wordpress --timeout=600s
    
    echo "Deployment Complete!"
    echo "WordPress URL: http://wordpress.${CI_PROJECT_PATH_SLUG}.demo.axual.com"
    echo "Admin URL: http://wordpress.${CI_PROJECT_PATH_SLUG}.demo.axual.com/wp-admin"
    echo "Username: admin"
    echo "Password: AdminPass123!"
  dependencies:
    - deploy_mysql
  only:
    - main
  retry:
    max: 2
    when:
      - script_failure

smoke_tests:
  stage: test
  script: |
    aws eks update-kubeconfig --region ${AWS_REGION} --name ${CLUSTER_NAME}
    kubectl cluster-info
    kubectl get pods -A
    kubectl get pods -l app.kubernetes.io/name=wordpress
    kubectl get pods -n mysql-operator
    kubectl get svc wordpress
    kubectl get svc -n mysql-operator my-mysql-innodbcluster
    kubectl get pvc -A
    
    # Check WordPress logs for errors
    echo "Checking WordPress logs..."
    kubectl logs -l app.kubernetes.io/name=wordpress --tail=50 || true
    
    echo "All smoke tests passed!"
  dependencies:
    - deploy_wordpress
  only:
    - main