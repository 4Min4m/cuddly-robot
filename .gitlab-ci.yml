stages:
  - validate
  - infrastructure
  - deploy
  - test

variables:
  CLUSTER_NAME: "axual-demo-cluster"
  AWS_REGION: "us-east-1"
  MYSQL_ROOT_PASSWORD: "DemoPassword123!" 

image: alpine:latest

before_script: |
  apk add --no-cache python3 py3-pip curl wget unzip bash ca-certificates
  pip3 install awscli --break-system-packages
  curl -LO "https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl"
  chmod +x kubectl && mv kubectl /usr/local/bin/
  curl -LO "https://get.helm.sh/helm-v3.17.0-linux-amd64.tar.gz"
  tar -zxvf helm-v3.17.0-linux-amd64.tar.gz && mv linux-amd64/helm /usr/local/bin/
  wget -O terraform.zip "https://releases.hashicorp.com/terraform/1.9.0/terraform_1.9.0_linux_amd64.zip"
  unzip -o terraform.zip -d /usr/local/bin/
  chmod +x /usr/local/bin/terraform
  aws configure set aws_access_key_id ${AWS_ACCESS_KEY_ID}
  aws configure set aws_secret_access_key ${AWS_SECRET_ACCESS_KEY}
  aws configure set region ${AWS_REGION}
  aws sts get-caller-identity

validate_terraform:
  stage: validate
  when: manual
  script: |
    cd terraform
    terraform init
    terraform validate
    set +e
    terraform plan -out=plan.tfplan -detailed-exitcode
    plan_exit=$?
    set -e
    if [ "$plan_exit" -eq 1 ]; then
      echo "Terraform plan failed due to configuration or credentials error."
      exit 1
    elif [ "$plan_exit" -ne 0 ] && [ "$plan_exit" -ne 2 ]; then
      echo "Terraform planning failed with unexpected exit code: $plan_exit."
      exit 1
    fi
    echo "Terraform plan succeeded(Exit code $plan_exit)."
  artifacts:
    paths:
      - terraform/plan.tfplan
    expire_in: 1 hour
  only:
    - main
    - merge_requests

deploy_infrastructure:
  stage: infrastructure
  when: manual
  script: |
    cd terraform
    terraform init
    terraform apply -input=false plan.tfplan
    echo "Waiting for EKS cluster to reach ACTIVE state..."
    aws eks wait cluster-active --name ${CLUSTER_NAME} --region ${AWS_REGION} || { 
      echo "EKS cluster failed to become active or timed out."
      exit 1
    }
    aws eks update-kubeconfig --region ${AWS_REGION} --name ${CLUSTER_NAME} --alias ${CLUSTER_NAME}
    kubectl cluster-info
    kubectl get nodes
  dependencies:
    - validate_terraform
  only:
    - main

deploy_prerequisites:
  stage: deploy
  script: |
    helm repo add eks https://aws.github.io/eks-charts
    helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
    helm repo add mysql-operator https://mysql.github.io/mysql-operator/
    helm repo add bitnami https://charts.bitnami.com/bitnami
    helm repo update
    aws eks update-kubeconfig --region ${AWS_REGION} --name ${CLUSTER_NAME}
    EBS_ROLE_ARN=$(aws iam get-role --role-name "${CLUSTER_NAME}-ebs-csi-driver" --query 'Role.Arn' --output text)
    ALB_ROLE_ARN=$(aws iam get-role --role-name "${CLUSTER_NAME}-alb-controller-role" --query 'Role.Arn' --output text)
    helm upgrade --install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver --namespace kube-system \
      --set controller.serviceAccount.create=true \
      --set controller.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="$EBS_ROLE_ARN" \
      --wait --timeout 5m
    helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller --namespace kube-system \
      --set clusterName=${CLUSTER_NAME} \
      --set region=${AWS_REGION} \
      --set serviceAccount.create=true \
      --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="$ALB_ROLE_ARN" \
      --wait --timeout 5m
    timeout 300s bash -c 'until kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver -o name 2>/dev/null | grep -q pod/; do echo "Waiting for aws-ebs-csi-driver pods..."; sleep 10; done'
    kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system --timeout=300s
    timeout 300s bash -c 'until kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o name 2>/dev/null | grep -q pod/; do echo "Waiting for aws-load-balancer-controller pods..."; sleep 10; done'
    kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=300s
  dependencies:
    - deploy_infrastructure
  only:
    - main

deploy_mysql:
  stage: deploy
  script: |
    helm repo add mysql-operator https://mysql.github.io/mysql-operator/
    helm repo update
    aws eks update-kubeconfig --region ${AWS_REGION} --name ${CLUSTER_NAME}
    helm upgrade --install mysql-operator mysql-operator/mysql-operator \
      --namespace mysql-operator \
      --create-namespace \
      --wait --timeout 10m
    kubectl get all -n mysql-operator
    kubectl wait --for=condition=available deployment/mysql-operator -n mysql-operator --timeout=300s
    kubectl wait --for=condition=ready pod -l name=mysql-operator -n mysql-operator --timeout=300s
    kubectl apply -f - <<EOF
apiVersion: mysql.oracle.com/v2
kind: InnoDBCluster
metadata:
  name: my-mysql-innodbcluster
  namespace: mysql-operator
spec:
  secretName: my-mysql-innodbcluster-secret
  instances: 1
  router:
    instances: 1
  tlsUseSelfSigned: true
  imagePullPolicy: IfNotPresent
EOF
    kubectl create secret generic my-mysql-innodbcluster-secret \
      --namespace mysql-operator \
      --from-literal=rootUser=root \
      --from-literal=rootHost=% \
      --from-literal=rootPassword="${MYSQL_ROOT_PASSWORD}" \
      --dry-run=client -o yaml | kubectl apply -f -
    kubectl get innodbcluster -n mysql-operator
    kubectl describe innodbcluster my-mysql-innodbcluster -n mysql-operator
    timeout 600s bash -c 'until kubectl get innodbcluster my-mysql-innodbcluster -n mysql-operator -o jsonpath="{.status.cluster.status}" 2>/dev/null | grep -q "ONLINE"; do 
      kubectl get innodbcluster my-mysql-innodbcluster -n mysql-operator -o jsonpath="{.status.cluster.status}" 2>/dev/null || echo "not ready"
      sleep 10
    done' || echo "Timeout waiting for cluster status"
    timeout 600s bash -c 'until kubectl get pods -n mysql-operator -l component=mysqld -o name 2>/dev/null | grep -q pod/; do 
      kubectl get pods -n mysql-operator
      sleep 10
    done'
    kubectl wait --for=condition=ready pod -n mysql-operator -l component=mysqld --timeout=600s || {
      kubectl get pods -n mysql-operator
      kubectl describe pods -n mysql-operator -l component=mysqld
    }
    timeout 120s bash -c 'until kubectl get svc my-mysql-innodbcluster -n mysql-operator 2>/dev/null; do echo "Waiting for service..."; sleep 5; done'
    kubectl get svc -n mysql-operator
    sleep 60
    for i in {1..10}; do
      if kubectl run -n mysql-operator mysql-connectivity-test \
        --image=mysql:8.0 \
        --rm -i \
        --restart=Never \
        -- mysql -h my-mysql-innodbcluster.mysql-operator.svc.cluster.local \
        -u root \
        -p${MYSQL_ROOT_PASSWORD} \
        -e "SELECT 1;"; then
        break
      else
        sleep 10
      fi
    done
    kubectl run -n mysql-operator mysql-init \
      --image=mysql:8.0 \
      --rm -i \
      --restart=Never \
      -- mysql -h my-mysql-innodbcluster.mysql-operator.svc.cluster.local \
      -u root \
      -p${MYSQL_ROOT_PASSWORD} \
      --execute="
        CREATE DATABASE IF NOT EXISTS wordpress;
        CREATE USER IF NOT EXISTS 'wordpress'@'%' IDENTIFIED BY 'wordpress123';
        GRANT ALL PRIVILEGES ON wordpress.* TO 'wordpress'@'%';
        FLUSH PRIVILEGES;
        SHOW DATABASES;
      "
    kubectl run -n mysql-operator mysql-verify \
      --image=mysql:8.0 \
      --rm -i \
      --restart=Never \
      -- mysql -h my-mysql-innodbcluster.mysql-operator.svc.cluster.local \
      -u wordpress \
      -pwordpress123 \
      -e "SHOW DATABASES; USE wordpress; SHOW TABLES;"
  dependencies:
    - deploy_prerequisites
  only:
    - main
  retry:
    max: 2
    when:
      - script_failure

deploy_wordpress:
  stage: deploy
  script: |
    helm repo add bitnami https://charts.bitnami.com/bitnami
    helm repo update
    aws eks update-kubeconfig --region ${AWS_REGION} --name ${CLUSTER_NAME}
    kubectl run -n mysql-operator mysql-check \
      --image=mysql:8.0 \
      --rm -i --restart=Never \
      --command -- \
      mysql -h my-mysql-innodbcluster.mysql-operator.svc.cluster.local \
      -u wordpress \
      -pwordpress123 \
      -e "SELECT 1;" || echo "MySQL check failed"
    if [ -f helm/wordpress-values.yaml ]; then
      cat helm/wordpress-values.yaml
    else
      echo "No custom wordpress-values.yaml found"
    fi
    if [ -f helm/wordpress-values.yaml ]; then
      helm upgrade --install wordpress bitnami/wordpress \
        --namespace default \
        --values helm/wordpress-values.yaml \
        --set ingress.hostname=wordpress.${CI_PROJECT_PATH_SLUG}.demo.axual.com \
        --wait --timeout 15m
    else
      helm upgrade --install wordpress bitnami/wordpress \
        --namespace default \
        --set mariadb.enabled=false \
        --set externalDatabase.host=my-mysql-innodbcluster.mysql-operator.svc.cluster.local \
        --set externalDatabase.port=3306 \
        --set externalDatabase.user=wordpress \
        --set externalDatabase.password=wordpress123 \
        --set externalDatabase.database=wordpress \
        --set wordpressUsername=admin \
        --set wordpressPassword=AdminPass123! \
        --set wordpressEmail=demo@axual.com \
        --set wordpressFirstName=Admin \
        --set wordpressLastName=User \
        --set wordpressBlogName="Axual Demo Blog" \
        --set ingress.enabled=true \
        --set ingress.ingressClassName=alb \
        --set ingress.hostname=wordpress.${CI_PROJECT_PATH_SLUG}.demo.axual.com \
        --set ingress.annotations."alb\.ingress\.kubernetes\.io/scheme"=internet-facing \
        --set ingress.annotations."alb\.ingress\.kubernetes\.io/target-type"=ip \
        --wait --timeout 15m
    fi
    timeout 600s bash -c 'until kubectl get pods -l app.kubernetes.io/name=wordpress -o name 2>/dev/null | grep -q pod/; do echo "Waiting for WordPress pods..."; sleep 10; done'
    kubectl get pods -l app.kubernetes.io/name=wordpress
    kubectl logs -l app.kubernetes.io/name=wordpress -c wordpress --tail=50 || echo "Could not fetch logs yet"
    kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=wordpress --timeout=600s
    echo "Deployment Complete!"
    echo "WordPress URL: http://wordpress.${CI_PROJECT_PATH_SLUG}.demo.axual.com"
    echo "Admin URL: http://wordpress.${CI_PROJECT_PATH_SLUG}.demo.axual.com/wp-admin"
    echo "Username: admin"
    echo "Password: AdminPass123!"
  dependencies:
    - deploy_mysql
  only:
    - main
  retry:
    max: 2
    when:
      - script_failure

smoke_tests:
  stage: test
  script: |
    aws eks update-kubeconfig --region ${AWS_REGION} --name ${CLUSTER_NAME}
    kubectl cluster-info
    kubectl get pods -A
    kubectl get pods -l app.kubernetes.io/name=wordpress
    kubectl get pods -n mysql-operator
    kubectl get svc wordpress
    kubectl get svc -n mysql-operator my-mysql-innodbcluster
    kubectl get pvc -A
    kubectl logs -l app.kubernetes.io/name=wordpress --tail=50 || true
    echo "All smoke tests passed!"
  dependencies:
    - deploy_wordpress
  only:
    - main